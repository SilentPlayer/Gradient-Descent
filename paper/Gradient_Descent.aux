\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{shalev}
\gdef\hy@title{Gradient descent based optimization algorithms}
\thanksnewlabel{e1@email}{{a01329095@unet.univie.ac.at}{1}}
\gdef\hy@author{a nonymous}
\gdef\hy@subject{Submitted to the Annals of the Department of Statistics and Operations Research }
\gdef\hy@keywords{}
\gdef\author@num{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{eq:objFunction}{{1.1}{1}{Introduction}{equation.1.1}{}}
\newlabel{eq:gd}{{1.2}{1}{Introduction}{equation.1.2}{}}
\newlabel{eq:gradient}{{1.3}{1}{Introduction}{equation.1.3}{}}
\citation{Deisenroth2020}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of $f(x) = x^2$ converging to it's minimum}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:gdex1}{{1}{2}{Example of $f(x) = x^2$ converging to it's minimum}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Types of gradient descent}{2}{section.2}\protected@file@percent }
\newlabel{eq:costFunction}{{2.1}{3}{Types of gradient descent}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Stochastic gradient descent - SGD}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Batch gradient descent}{3}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces contourplot of a function where the red path comes from batch gradient descent and blue from SGD}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:contour}{{2}{3}{contourplot of a function where the red path comes from batch gradient descent and blue from SGD}{figure.2}{}}
\citation{geron2019hands}
\citation{polyak}
\citation{Aggarwal18}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Mini-batch gradient descent}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient descent based optimization algorithms}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Momentum based GD}{4}{subsection.3.1}\protected@file@percent }
\newlabel{eq:mgd1}{{3.1}{4}{Momentum based GD}{equation.3.1}{}}
\newlabel{eq:mgd2}{{3.2}{4}{Momentum based GD}{equation.3.2}{}}
\newlabel{adagrad}{{3.2}{4}{Adagrad}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Adagrad}{4}{subsection.3.2}\protected@file@percent }
\newlabel{eq:adagrad1}{{3.3}{4}{Adagrad}{equation.3.3}{}}
\citation{RiedmillerM1993Adam}
\newlabel{eq:adagrad2}{{3.4}{5}{Adagrad}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}RProp}{5}{subsection.3.3}\protected@file@percent }
\newlabel{eq:rprop1}{{3.5}{5}{RProp}{equation.3.5}{}}
\newlabel{eq:rprop2}{{3.6}{5}{RProp}{equation.3.6}{}}
\citation{adam}
\citation{geron2019hands}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}RMSprop}{6}{subsection.3.4}\protected@file@percent }
\newlabel{eq:rmsprop1}{{3.7}{6}{RMSprop}{equation.3.7}{}}
\newlabel{eq:rmsprop2}{{3.8}{6}{RMSprop}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Adam}{6}{subsection.3.5}\protected@file@percent }
\newlabel{eq:adam1}{{3.9}{6}{Adam}{equation.3.9}{}}
\newlabel{eq:adam2}{{3.10}{6}{Adam}{equation.3.10}{}}
\newlabel{eq:adam3}{{3.11}{6}{Adam}{equation.3.11}{}}
\newlabel{eq:adam4}{{3.12}{6}{Adam}{equation.3.12}{}}
\newlabel{eq:adam5}{{3.13}{6}{Adam}{equation.3.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Comparing the different algorithms}{7}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Graphical method}{7}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Simulations}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 2D-contpourplot of a bowl shaped function displaying the paths of different algorithms}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:contour1}{{3}{8}{2D-contpourplot of a bowl shaped function displaying the paths of different algorithms}{figure.3}{}}
\newlabel{eq:model}{{4.2}{8}{Simulations}{equation.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 3D-contpourplot of a bowl shaped function displaying the paths of different algorithms}}{9}{figure.4}\protected@file@percent }
\newlabel{fig:contour2}{{4}{9}{3D-contpourplot of a bowl shaped function displaying the paths of different algorithms}{figure.4}{}}
\bibstyle{imsart-nameyear}
\bibdata{lit}
\bibcite{Aggarwal18}{{1}{2018}{{Aggarwal}}{{}}}
\bibcite{Deisenroth2020}{{2}{2020}{{Deisenroth, Faisal and Ong}}{{}}}
\bibcite{geron2019hands}{{3}{2019}{{G{\'e}ron}}{{}}}
\bibcite{adam}{{4}{2014}{{Kingma and Ba}}{{}}}
\bibcite{polyak}{{5}{1964}{{Polyak}}{{}}}
\bibcite{RiedmillerM1993Adam}{{6}{1993}{{Riedmiller and Braun}}{{}}}
\bibcite{shalev}{{7}{2014}{{Shalev-Shwartz and Ben-David}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Acknowledgements}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{10}{section*.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparision of the different gradient based optimization algorithms, with learning rate 0.01}}{11}{figure.5}\protected@file@percent }
\newlabel{fig:simulation1}{{5}{11}{Comparision of the different gradient based optimization algorithms, with learning rate 0.01}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Summary statistics for simulations with adam with a learning rate of 0.01.}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:sumstat1}{{6}{11}{Summary statistics for simulations with adam with a learning rate of 0.01}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparision of the different gradient based optimization algorithms, with learning rate 0.1}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:simulation2}{{7}{12}{Comparision of the different gradient based optimization algorithms, with learning rate 0.1}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Summary statistics for simulations with adam with a learning rate of 0.1.}}{12}{figure.8}\protected@file@percent }
\newlabel{fig:sumstat2}{{8}{12}{Summary statistics for simulations with adam with a learning rate of 0.1}{figure.8}{}}
\gdef \@abspage@last{12}
