@book{shalev,
  author = {Shalev-Shwartz, Shai and Ben-David, Shai},
  biburl = {https://www.bibsonomy.org/bibtex/293329d1cd5964dd826bba3100cd17fe4/dblp},
  ee = {http://www.cambridge.org/de/academic/subjects/computer-science/pattern-recognition-and-machine-learning/understanding-machine-learning-theory-algorithms},
  interhash = {125d708c7b440a3cfeb6146e83ab5de3},
  intrahash = {93329d1cd5964dd826bba3100cd17fe4},
  isbn = {978-1-10-705713-5},
  keywords = {dblp},
  pages = {I-XVI, 1-397},
  publisher = {Cambridge University Press},
  timestamp = {2020-06-06T11:43:42.000+0200},
  title = {Understanding Machine Learning - From Theory to Algorithms.},
  year = {2014}
}

@book{Deisenroth2020,
  added-at = {2019-10-12T23:39:32.000+0200},
  author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
  biburl = {https://www.bibsonomy.org/bibtex/271e556439bb49b11e015aa4c0d9cb785/lopusz_kdd},
  description = {Mathematics for Machine Learning: Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong: 9781108455145: Amazon.com: Books},
  interhash = {876f4a593e888f3257674a89d7456f25},
  intrahash = {71e556439bb49b11e015aa4c0d9cb785},
  keywords = {general_machine_learning},
  publisher = {Cambridge University Press},
  timestamp = {2019-10-12T23:43:51.000+0200},
  title = {Mathematics for Machine Learning},
  year = {2020}
}

@book{geron2019hands,
  title={Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
  author={G{\'e}ron, A.},
  isbn={9781492032618},
  url={https://books.google.at/books?id=HHetDwAAQBAJ},
  year={2019},
  publisher={O'Reilly Media}
}

@article{polyak,
author = {Polyak, Boris},
year = {1964},
month = {12},
pages = {1-17},
title = {Some methods of speeding up the convergence of iteration methods},
volume = {4},
journal = {Ussr Computational Mathematics and Mathematical Physics},
doi = {10.1016/0041-5553(64)90137-5}
}


@article{journals/jmlr/DuchiHS11,
  added-at = {2019-07-10T00:00:00.000+0200},
  author = {Duchi, John C. and Hazan, Elad and Singer, Yoram},
  biburl = {https://www.bibsonomy.org/bibtex/24c8a0f958df17cd3520d9d4032fbdbdf/dblp},
  ee = {http://dl.acm.org/citation.cfm?id=2021068},
  interhash = {d2bb1dcfcc9549a93e0b1f0dd8d23cf9},
  intrahash = {4c8a0f958df17cd3520d9d4032fbdbdf},
  journal = {J. Mach. Learn. Res.},
  keywords = {dblp},
  pages = {2121-2159},
  timestamp = {2019-07-11T11:41:54.000+0200},
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlr12.html#DuchiHS11},
  volume = 12,
  year = 2011
}

@book{Aggarwal18,
  __markedentry = {[flint:2]},
  abstract = {This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories. First, the basics of neural networks: Many traditional machine learning models can be understood as special cases of neural networks. An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. The second part deals with Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. The final part covers Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10.},
  added-at = {2019-03-30T20:36:26.000+0100},
  address = {Cham},
  author = {Aggarwal, Charu C.},
  biburl = {https://www.bibsonomy.org/bibtex/26681a2e990d3d35887d519e2ec3bd7a9/flint63},
  doi = {10.1007/978-3-319-94463-0},
  file = {copy:201x/8/Aggarwal18.pdf:PDF;shop:https\://www.springer.com/978-3-319-94462-3:text/html;archive:https\://www.springerprofessional.de/neural-networks-and-deep-learning/16073524:text/html},
  gender = {sm},
  interhash = {59b0c6aa4a43b74401bca82051990b53},
  intrahash = {6681a2e990d3d35887d519e2ec3bd7a9},
  isbn = {978-3-319-94462-3},
  keywords = {02001 103 8book 9springer ai algorithm learn neuro numerical},
  owner = {flint},
  pages = 497,
  publisher = {Springer},
  referencetype = {book},
  subtitle = {A Textbook},
  timestamp = {2019-03-30T20:36:26.000+0100},
  title = {Neural Networks and Deep Learning},
  x.asin = {3319944622},
  x.sortdate = {2018-09},
  year = 2018
}

@inproceedings{RiedmillerM1993Adam,
abstract = {A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of RPROP are shown in comparison to other adaptive techniques.< >},
pages = {586--591 vol.1},
publisher = {IEEE},
booktitle = {IEEE International Conference on Neural Networks},
isbn = {0780309995},
year = {1993},
title = {A direct adaptive method for faster backpropagation learning: the RPROP algorithm},
language = {eng},
author = {Riedmiller, M and Braun, H},
keywords = {Acceleration ; Backpropagation algorithms ; Computer networks ; Convergence ; Feedforward systems ; Neurons ; Supervised learning ; Writing},
}


@misc{adam,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license},
}

